{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.01 s, sys: 92 ms, total: 2.1 s\n",
      "Wall time: 2.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# import necessary libraries\n",
    "from sklearn.preprocessing import Imputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, metrics\n",
    "# from sklearn.grid_search import GridSearchCV\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# read data from file\n",
    "train_set = pd.read_csv(\"train-data.txt\")\n",
    "test_set = pd.read_csv(\"testx.csv\")\n",
    "# print(train_set.shape,test_set.shape)\n",
    "\n",
    "# add columns for data set\n",
    "col_labels = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'wage_class']\n",
    "train_set.columns = col_labels\n",
    "test_set.columns = col_labels[:-1]\n",
    "\n",
    "# combine two data set\n",
    "def id(a):\n",
    "    for i in range(14):\n",
    "        if a == col_labels[i]:\n",
    "            return i\n",
    "combined_set = pd.concat([train_set, test_set], axis = 0,sort=False)\n",
    "isMissing = np.zeros(combined_set.shape) # mark the missing data location\n",
    "cnum = 0\n",
    "for col in col_labels:\n",
    "    if col == 'wage_class':\n",
    "        break\n",
    "    t = list(combined_set[col])\n",
    "    for i in range(38560):\n",
    "        if t[i] == ' ?':\n",
    "            isMissing[i,cnum] = 1\n",
    "    cnum += 1\n",
    "combined_set = combined_set.replace(' ?',np.nan)\n",
    "\n",
    "sum(sum(isMissing))\n",
    "\n",
    "# mark the reflection of wage_class\n",
    "list(combined_set['wage_class'])[0]\n",
    "\n",
    "# replace strings with integers and \n",
    "# mark the way to fix data set 1 means using mode while 0 means using means\n",
    "typ = [0 for i in range(14)]\n",
    "cnt = 0\n",
    "for feature in combined_set.columns:\n",
    "    if combined_set[feature].dtype == 'object':\n",
    "        typ[cnt] = 1\n",
    "        cnt += 1\n",
    "        combined_set[feature] = pd.Categorical(combined_set[feature]).codes\n",
    "\n",
    "# mark the reflection of wage_class\n",
    "list(combined_set['wage_class'])[0]\n",
    "\n",
    "# test\n",
    "s = set([])\n",
    "cnum = 0\n",
    "for col in col_labels:\n",
    "    if col == 'wage_class':\n",
    "        break\n",
    "    t = list(combined_set[col])\n",
    "    for i in range(38560):\n",
    "        if isMissing[i,cnum] > 0:\n",
    "            s.update([t[i]])\n",
    "    if len(s) > 1:\n",
    "        break\n",
    "    cnum += 1\n",
    "s\n",
    "\n",
    "# repleace -1 with np.nan\n",
    "cnum = 0\n",
    "cnt = 0\n",
    "for col in col_labels:\n",
    "    if col == 'wage_class':\n",
    "        break\n",
    "    t = list(combined_set[col])\n",
    "    for i in range(38560):\n",
    "        if isMissing[i,cnum] > 0:\n",
    "            t[i] = np.nan\n",
    "            cnt += 1\n",
    "    combined_set[col] = t\n",
    "    cnum += 1\n",
    "# print(cnt)\n",
    "\n",
    "# fix the data set\n",
    "for i in range(14):\n",
    "    if typ[i] == 1:\n",
    "        combined_set[col_labels[i]]=combined_set[col_labels[i]].fillna(combined_set[col_labels[i]].mode().iloc[0])\n",
    "    else:\n",
    "        combined_set[col_labels[i]]=combined_set[col_labels[i]].fillna(combined_set[col_labels[i]].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7841, 15) (24719, 15)\n",
      "(34606, 14)\n"
     ]
    }
   ],
   "source": [
    "train = combined_set[:32560]\n",
    "p_train = train[train['wage_class'] > 0]\n",
    "n_train = train[train['wage_class'] == 0]\n",
    "print(p_train.shape,n_train.shape)\n",
    "#train_x = combined_set.drop(columns=['wage_class'])[:32560]\n",
    "#train_y = combined_set.drop(columns = col_labels[:-1])[:32560]\n",
    "test_x = combined_set.drop(columns=['wage_class'])[32560:]\n",
    "# print(train_x.shape,train_y.shape,test_x.shape)\n",
    "\n",
    "# data process done\n",
    "x = train.iloc[:, :-1] # 切片，得到输入x\n",
    "y = train.iloc[:, -1] # 切片，得到标签y\n",
    "model_smote = SMOTE(sampling_strategy=0.4) # 建立SMOTE模型对象\n",
    "x_smote_resampled, y_smote_resampled = model_smote.fit_sample(x,y) # 输入数据并作过抽样处理\n",
    "x_smote_resampled = pd.DataFrame(x_smote_resampled, columns=col_labels[:-1]) # 将数据转换为数据框并命名列名\n",
    "y_smote_resampled = pd.DataFrame(y_smote_resampled,columns=['wage_class']) # 将数据转换为数据框并命名列名\n",
    "smote_resampled = pd.concat([x_smote_resampled, y_smote_resampled],axis=1) # 按列合并数据框\n",
    "groupby_data_smote = smote_resampled.groupby('wage_class').count() # 对label做分类汇总\n",
    "#print (groupby_data_smote) # 打印输出经过SMOTE处理后的数据集样本分类分布\n",
    "#print(x_smote_resampled.shape,y_smote_resampled.shape)\n",
    "# split the data set\n",
    "#train_x = combined_set.drop(columns=['wage_class'])[:32560]\n",
    "#train_y = combined_set.drop(columns = col_labels[:-1])[:32560]\n",
    "#test_x = combined_set.drop(columns=['wage_class'])[32560:]\n",
    "# print(train_x.shape,train_y.shape,test_x.shape)\n",
    "train_x = x_smote_resampled\n",
    "train_y = y_smote_resampled\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boostingTrain_x = pd.DataFrame([],columns=col_labels[:-1],dtype='int64')\n",
    "#boostingTrain_y = pd.DataFrame([],columns=col_labels[14:],dtype='int64')\n",
    "#train_x['sex'] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7722664735698769\n"
     ]
    }
   ],
   "source": [
    "sumans = np.zeros(6000,dtype = 'int64')\n",
    "arr = np.array([[2, 1, 2, 2, 0, 1, 2],\n",
    "    [2, 2, 0, 0, 1, 0, 0],\n",
    "    [1, 0, 2, 1, 0, 2, 1],\n",
    "    [2, 0, 2, 1, 1, 1, 0],\n",
    "    [0, 0, 1, 0, 1, 0, 0],\n",
    "    [2, 1, 2, 2, 0, 1, 2],\n",
    "    [2, 2, 0, 0, 1, 0, 0],\n",
    "    [1, 0, 2, 1, 0, 2, 1],\n",
    "    [2, 0, 2, 1, 1, 1, 0],\n",
    "    [0, 0, 1, 0, 1, 0, 0]])\n",
    "def fun(par,num_rounds):\n",
    "    # generate random train set and valid set\n",
    "    X_train,X_valid,y_train,y_valid = train_test_split(\n",
    "        train_x, train_y, test_size=0.3)\n",
    "    Dtrain=xgb.DMatrix(X_train,y_train)\n",
    "\n",
    "    # train model\n",
    "    plst=par.items()\n",
    "    model=xgb.train(plst,Dtrain,num_rounds)\n",
    "\n",
    "    # predict valid data set\n",
    "    dvalid = xgb.DMatrix(X_valid)\n",
    "    ans = model.predict(dvalid)\n",
    "    y_valid1 = list(y_valid['wage_class'])\n",
    "    dvalid = xgb.DMatrix(X_valid)\n",
    "    ans = model.predict(dvalid)\n",
    "\n",
    "    # calculate the F1 score\n",
    "    length = len(y_valid)\n",
    "    tp = tn = fp = fn = cnt = 0\n",
    "    for i in range(length):\n",
    "        p,t = ans[i],y_valid1[i]\n",
    "        if p == 1 and t == 1:\n",
    "            tp += 1\n",
    "        elif p == 0 and t == 0:\n",
    "            tn += 1\n",
    "        elif p == 1 and t == 0:\n",
    "            fp += 1\n",
    "        elif p == 0 and t == 1:\n",
    "            fn += 1\n",
    "    pp = tp / (tp + fp)\n",
    "    rr = tp / (tp + fn)\n",
    "    f1 = 2 * (pp * rr) / (pp + rr)\n",
    "    test = xgb.DMatrix(test_x)\n",
    "    final = np.array(model.predict(test),dtype='int64')\n",
    "    global sumans\n",
    "    sumans = sumans + final\n",
    "    pos = 0\n",
    "    for i in range(len(final)):\n",
    "        if final[i] == 1:\n",
    "            pos += 1\n",
    "    #print(pos / len(final))\n",
    "    final = pd.DataFrame({'y':final})\n",
    "    file = pd.concat([final],sort=False)\n",
    "    file.to_csv(\"ans.csv\",index=False,sep=',')\n",
    "    # calculate the precision\n",
    "    #for i in range(length):\n",
    "    #    if ans[i] == y_valid1[i]:\n",
    "    #        cnt += 1\n",
    "    #precision = cnt / length\n",
    "    # print(precision)\n",
    "\n",
    "    print(f1)\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 2,\n",
    "    'gamma': 0.5,# 0 ~ 1 0.5 3\n",
    "    'max_depth': 5,# 5 ~ 7 3\n",
    "    'lambda': 1,\n",
    "    'subsample': 0.8,# 0.6 ~ 0.8 0.1 3\n",
    "    'colsample_bytree': 0.6,# 0.6 ~ 0.8 0.1 3\n",
    "    'min_child_weight': 2, # 2 ~ 4 1 3\n",
    "    'eta': 0.11, # 0.09 ~ 0.11 0.01 3\n",
    "#    'seed': 1000,\n",
    "#    nthread : 4\n",
    "    'silent': 1,\n",
    "}\n",
    "\n",
    "delta = {}\n",
    "delta['gamma'] = np.linspace(0,1,num=3)\n",
    "delta['max_depth'] = np.linspace(5,7,num=3,dtype='int64')\n",
    "delta['subsample'] = np.linspace(0.6,0.8,num=3)\n",
    "delta['colsample_bytree'] = np.linspace(0.6,0.8,num=3)\n",
    "delta['min_child_weight'] = np.linspace(2,4,num=3,dtype='int64')\n",
    "delta['eta'] = np.linspace(0.09,0.11,num=3)\n",
    "delta['num_rounds'] = np.linspace(190,210,num=3,dtype='int64')\n",
    "\n",
    "for i in range(1):\n",
    "    [l,m,n,o,p,q,r] = arr[i]\n",
    "    params['gamma'] = delta['gamma'][l]\n",
    "    params['max_depth'] = delta['max_depth'][m]\n",
    "    params['subsample'] = delta['subsample'][n]\n",
    "    params['colsample_bytree'] = delta['colsample_bytree'][o]\n",
    "    params['min_child_weight'] = delta['min_child_weight'][p]\n",
    "    params['eta'] = delta['eta'][q]\n",
    "    fun(params,delta['num_rounds'][r])\n",
    "\n",
    "sumans = pd.DataFrame({'y':sumans})\n",
    "file = pd.concat([sumans],sort=False)\n",
    "file.to_csv(\"ans.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
